# üîÑ Event Sync: Mecanismo de Resiliencia y Re-intentos

## üìã Resumen

El sistema de inventario implementa un **mecanismo de resiliencia autom√°tica** para garantizar la entrega eventual de eventos al message broker (Redis/Kafka), incluso si el broker est√° temporalmente ca√≠do.

## üéØ Problema que Resuelve

**Escenario**: Redis est√° ca√≠do temporalmente

```
StockService.UpdateStock()
    ‚Üì
‚úÖ Evento guardado en DB (auditor√≠a garantizada)
    ‚Üì
‚ùå Publicaci√≥n a Redis FALLA (connection refused)
    ‚Üì
‚ö†Ô∏è  Sin mecanismo de retry ‚Üí EVENTO PERDIDO
```

**Soluci√≥n**: EventSyncService con re-intentos autom√°ticos

```
StockService.UpdateStock()
    ‚Üì
‚úÖ Evento guardado en DB (synced_at = NULL)
    ‚Üì
‚ùå Publicaci√≥n a Redis FALLA
    ‚Üì
[10 segundos despu√©s - EventSyncWorker ejecuta]
    ‚Üì
‚úÖ Re-intenta publicar eventos pendientes
    ‚Üì
‚úÖ Marca synced_at = NOW() cuando tiene √©xito
```

## üèóÔ∏è Arquitectura del Sistema

### Componentes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Capa de Servicios                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ  StockService   ‚îÇ        ‚îÇReservationService‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ           ‚îÇ                          ‚îÇ                      ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                      ‚îÇ                                      ‚îÇ
‚îÇ                      ‚ñº                                      ‚îÇ
‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ           ‚îÇ  EventPublisher    ‚îÇ (Interface)               ‚îÇ
‚îÇ           ‚îÇ  - Publish()       ‚îÇ                           ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò                           ‚îÇ
‚îÇ                  ‚îÇ          ‚îÇ                               ‚îÇ
‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ        ‚ñº                                 ‚ñº                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇRedis     ‚îÇ                    ‚îÇEventSyncService‚îÇ         ‚îÇ
‚îÇ  ‚îÇPublisher ‚îÇ                    ‚îÇ(Retry Logic)   ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ       ‚îÇ                                  ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                  ‚îÇ
        ‚îÇ                                  ‚îÇ
        ‚ñº                                  ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Redis   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   DB     ‚îÇ
  ‚îÇ Streams  ‚îÇ   Re-publish failed  ‚îÇ events   ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   events from DB     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PASO 1: Operaci√≥n de Negocio                               ‚îÇ
‚îÇ UpdateStock() / CreateReservation()                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PASO 2: Persistencia en DB                                 ‚îÇ
‚îÇ - event.Save(db)                                           ‚îÇ
‚îÇ - synced_at = NULL (pendiente de publicaci√≥n)             ‚îÇ
‚îÇ ‚úÖ Auditor√≠a GARANTIZADA (siempre se guarda)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PASO 3: Intento de Publicaci√≥n Inmediata                   ‚îÇ
‚îÇ publisher.Publish(event)                                   ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚úÖ √âXITO  ‚Üí synced_at = NOW()                              ‚îÇ
‚îÇ ‚ùå FALLA  ‚Üí synced_at = NULL (queda pendiente)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº (si fall√≥)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PASO 4: Background Worker (cada 10 segundos)               ‚îÇ
‚îÇ EventSyncWorker ejecuta:                                   ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ 1. eventRepo.GetPendingEvents() ‚Üí WHERE synced_at IS NULL ‚îÇ
‚îÇ 2. Para cada evento:                                      ‚îÇ
‚îÇ    - publisher.Publish(event)                             ‚îÇ
‚îÇ    - Si √©xito ‚Üí Mark synced_at = NOW()                    ‚îÇ
‚îÇ    - Si falla ‚Üí Dejar NULL (retry en pr√≥xima ejecuci√≥n)   ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚úÖ Garantiza entrega EVENTUAL                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä Tabla de Componentes

| Componente | Archivo | Responsabilidad | Frecuencia |
|------------|---------|----------------|------------|
| **StockService** | `internal/service/stock_service.go` | Publicaci√≥n directa (tiempo real) | Por operaci√≥n |
| **ReservationService** | `internal/service/reservation_service.go` | Publicaci√≥n directa (tiempo real) | Por operaci√≥n |
| **EventSyncService** | `internal/service/event_sync_service.go` | Re-intentos de eventos fallidos | Cada 10s (worker) |
| **EventSyncWorker** | `cmd/api/main.go:243` | Ejecuta SyncPendingEvents() | Background (cada 10s) |
| **EventRepository** | `internal/repository/event_repository.go` | Tracking de synced_at | Por evento |
| **EventPublisher** | `internal/infrastructure/*_publisher.go` | Abstracci√≥n de brokers | Variable |

## üîß Implementaci√≥n

### EventSyncService (C√≥digo Clave)

```go
// internal/service/event_sync_service.go

type EventSyncService struct {
    eventRepo *repository.EventRepository
    publisher EventPublisher  // ‚úÖ Inyectado para re-intentos
}

func (s *EventSyncService) SyncPendingEvents(ctx context.Context, batchSize int) (int, error) {
    // 1. Obtener eventos NO sincronizados (synced_at IS NULL)
    events, err := s.eventRepo.GetPendingEvents(ctx, batchSize)
    if err != nil {
        return 0, fmt.Errorf("failed to get pending events: %w", err)
    }

    syncedCount := 0
    failedCount := 0
    eventIDs := make([]string, 0, len(events))

    // 2. RE-INTENTAR publicaci√≥n de cada evento
    for _, event := range events {
        err := s.publisher.Publish(ctx, event)
        if err != nil {
            log.Printf("‚ö†Ô∏è  Failed to sync event %s: %v (will retry later)", event.ID, err)
            failedCount++
            continue  // No marcar como sincronizado
        }

        eventIDs = append(eventIDs, event.ID)
        syncedCount++
    }

    // 3. Marcar como sincronizados SOLO los exitosos
    if len(eventIDs) > 0 {
        err = s.eventRepo.MarkMultipleAsSynced(ctx, eventIDs)
        if err != nil {
            return syncedCount, fmt.Errorf("failed to mark events as synced: %w", err)
        }
        log.Printf("‚úÖ Successfully synced %d events (failed: %d)", syncedCount, failedCount)
    }

    return syncedCount, nil
}
```

### Worker en main.go

```go
// cmd/api/main.go:243

func startEventSyncWorker(service *service.EventSyncService) {
    ticker := time.NewTicker(10 * time.Second)  // Ejecuta cada 10 segundos
    defer ticker.Stop()

    log.Println("üì° Event synchronization worker started")

    for range ticker.C {
        ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
        count, err := service.SyncPendingEvents(ctx, 100)  // Batch de 100 eventos
        cancel()

        if err != nil {
            log.Printf("Error syncing events: %v", err)
        } else if count > 0 {
            log.Printf("‚úÖ Synced %d events", count)
        }
    }
}
```

## üìà Ventajas del Dise√±o

| Ventaja | Descripci√≥n | Beneficio |
|---------|-------------|-----------|
| **Auditor√≠a Garantizada** | Eventos SIEMPRE se guardan en DB | No se pierde informaci√≥n hist√≥rica |
| **Resiliencia Autom√°tica** | Worker re-intenta sin intervenci√≥n manual | Sin downtime por ca√≠das temporales de Redis |
| **Sin P√©rdida de Datos** | Eventos pendientes se publican cuando broker vuelve | Consistencia eventual garantizada |
| **Observabilidad** | Campo `synced_at` permite monitorear lag | M√©tricas y alertas f√°ciles de implementar |
| **Idempotencia** | Re-publicar es seguro (event IDs √∫nicos) | No hay eventos duplicados |
| **Escalabilidad** | Batch processing (100 eventos/ciclo) | Maneja grandes vol√∫menes de eventos pendientes |

## üß™ Testing

### Casos de Prueba Implementados

```bash
# Ejecutar tests de resiliencia
go test ./test/unit -v -run TestEventSyncService_RetryMechanism

# Tests implementados:
# ‚úÖ Retry_PublishFailedEvents_Success
#    - Simula Redis ca√≠do en primer intento
#    - Verifica que evento queda pendiente
#    - Segundo intento tiene √©xito
#    - Evento se marca como sincronizado

# ‚úÖ Retry_PartialFailure_OnlySuccessfulMarked
#    - 4 eventos: 2 fallan, 2 tienen √©xito
#    - Verifica que SOLO los exitosos se marcan como synced
#    - Los fallidos quedan pendientes para pr√≥ximo retry

# ‚úÖ Retry_NoOpPublisher_AlwaysSucceeds
#    - NoOpPublisher nunca falla
#    - Todos los eventos se sincronizan en primer intento
```

### Ejemplo de Logs

```bash
# ‚úÖ Publicaci√≥n exitosa (tiempo real)
‚úÖ Event published to Redis: evt-20251028150405-001 (stock.updated)
‚úÖ Event synced to DB: evt-20251028150405-001

# ‚ùå Redis ca√≠do (se guarda en DB, publicaci√≥n falla)
‚úÖ Event saved to DB: evt-20251028150406-002 (stock.created)
‚ö†Ô∏è  Failed to publish to Redis: connection refused (will retry)

# üîÑ Worker re-intenta 10 segundos despu√©s (Redis sigue ca√≠do)
üì° Event synchronization worker started
‚ö†Ô∏è  Failed to sync event evt-20251028150406-002: connection refused (will retry later)
‚ö†Ô∏è  All 1 events failed to sync (will retry in next cycle)

# ‚úÖ Redis vuelve, evento se publica exitosamente
‚úÖ Successfully synced 1 events (failed: 0)
‚úÖ Event published to Redis: evt-20251028150406-002 (stock.created)
```

## üîç Monitoreo y Observabilidad

### Consultas SQL √ötiles

```sql
-- Ver eventos pendientes de sincronizaci√≥n
SELECT 
    id,
    event_type,
    aggregate_id,
    store_id,
    created_at,
    ROUND((julianday('now') - julianday(created_at)) * 24 * 60, 2) as minutes_pending
FROM events
WHERE synced_at IS NULL
ORDER BY created_at DESC;

-- Contar eventos pendientes por tipo
SELECT 
    event_type,
    COUNT(*) as pending_count,
    MIN(created_at) as oldest_pending
FROM events
WHERE synced_at IS NULL
GROUP BY event_type
ORDER BY pending_count DESC;

-- Latencia de sincronizaci√≥n (promedio)
SELECT 
    event_type,
    AVG((julianday(synced_at) - julianday(created_at)) * 24 * 60) as avg_sync_latency_minutes
FROM events
WHERE synced_at IS NOT NULL
GROUP BY event_type;

-- Eventos fallidos hace m√°s de 1 hora (alerta)
SELECT COUNT(*) as critical_pending
FROM events
WHERE synced_at IS NULL
  AND datetime(created_at) < datetime('now', '-1 hour');
```

### M√©tricas Recomendadas (Prometheus)

```go
// M√©tricas sugeridas para implementaci√≥n futura

// Contador de eventos pendientes
eventsPendingGauge.Set(float64(pendingCount))

// Latencia de sincronizaci√≥n
syncLatencyHistogram.Observe(syncDuration.Seconds())

// Eventos sincronizados exitosamente
eventsSyncedCounter.Inc()

// Eventos fallidos
eventsFailedCounter.Inc()
```

## üöÄ Configuraci√≥n

### Variables de Entorno

```bash
# Frecuencia del worker (no configurable actualmente, hardcoded a 10s)
# Futuro: EVENT_SYNC_INTERVAL=10s

# Tama√±o de batch
# Futuro: EVENT_SYNC_BATCH_SIZE=100

# Timeout de sincronizaci√≥n
# Futuro: EVENT_SYNC_TIMEOUT=30s
```

### Escalabilidad

| Escenario | Configuraci√≥n Recomendada |
|-----------|---------------------------|
| **Bajo Volumen** (<100 eventos/min) | Intervalo: 10s, Batch: 100 |
| **Medio Volumen** (100-1000 eventos/min) | Intervalo: 5s, Batch: 500 |
| **Alto Volumen** (>1000 eventos/min) | Intervalo: 1s, Batch: 1000, Worker dedicado |

## üîÆ Roadmap Futuro

- [ ] Configuraci√≥n de intervalo via env var (`EVENT_SYNC_INTERVAL`)
- [ ] M√©tricas de Prometheus integradas
- [ ] Alertas cuando eventos pendientes > umbral
- [ ] Dashboard de Grafana con visualizaci√≥n de lag
- [ ] Dead Letter Queue para eventos fallidos >N veces
- [ ] Priorizaci√≥n de eventos cr√≠ticos vs. no cr√≠ticos
- [ ] Compresi√≥n de eventos antes de publicar (batch)
- [ ] Circuit Breaker para evitar reintentos innecesarios

## üìö Referencias

- [Event Sourcing Pattern](https://martinfowler.com/eaaDev/EventSourcing.html)
- [Outbox Pattern](https://microservices.io/patterns/data/transactional-outbox.html)
- [Retry Patterns](https://docs.microsoft.com/en-us/azure/architecture/patterns/retry)

---

**Autor**: Sistema de Inventario Distribuido  
**Fecha**: Octubre 2025  
**Versi√≥n**: 1.0.0
